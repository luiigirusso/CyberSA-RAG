{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "54b83f29-aaac-48c5-96d0-c7306a4829c4",
      "metadata": {
        "id": "54b83f29-aaac-48c5-96d0-c7306a4829c4"
      },
      "source": [
        "# Lesson 4: Constructing a Knowledge Graph from Text Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a87f162a-036e-4f92-b747-7ccfc6dc7b70",
      "metadata": {
        "id": "a87f162a-036e-4f92-b747-7ccfc6dc7b70"
      },
      "source": [
        "<p style=\"background-color:#fd4a6180; padding:15px; margin-left:20px\"> <b>Note:</b> This notebook takes about 30 seconds to be ready to use. Please wait until the \"Kernel starting, please wait...\" message clears from the top of the notebook before running any cells. You may start the video while you wait.</p>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r CyberSA-RAG/"
      ],
      "metadata": {
        "id": "9U2CXVMIqs4F"
      },
      "id": "9U2CXVMIqs4F",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/luiigirusso/CyberSA-RAG.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aecUNM9VpHI8",
        "outputId": "bf981856-548d-41ae-d03a-d0b16cbda20b"
      },
      "id": "aecUNM9VpHI8",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CyberSA-RAG'...\n",
            "remote: Enumerating objects: 190, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/190)\u001b[K\rremote: Counting objects:   1% (2/190)\u001b[K\rremote: Counting objects:   2% (4/190)\u001b[K\rremote: Counting objects:   3% (6/190)\u001b[K\rremote: Counting objects:   4% (8/190)\u001b[K\rremote: Counting objects:   5% (10/190)\u001b[K\rremote: Counting objects:   6% (12/190)\u001b[K\rremote: Counting objects:   7% (14/190)\u001b[K\rremote: Counting objects:   8% (16/190)\u001b[K\rremote: Counting objects:   9% (18/190)\u001b[K\rremote: Counting objects:  10% (19/190)\u001b[K\rremote: Counting objects:  11% (21/190)\u001b[K\rremote: Counting objects:  12% (23/190)\u001b[K\rremote: Counting objects:  13% (25/190)\u001b[K\rremote: Counting objects:  14% (27/190)\u001b[K\rremote: Counting objects:  15% (29/190)\u001b[K\rremote: Counting objects:  16% (31/190)\u001b[K\rremote: Counting objects:  17% (33/190)\u001b[K\rremote: Counting objects:  18% (35/190)\u001b[K\rremote: Counting objects:  19% (37/190)\u001b[K\rremote: Counting objects:  20% (38/190)\u001b[K\rremote: Counting objects:  21% (40/190)\u001b[K\rremote: Counting objects:  22% (42/190)\u001b[K\rremote: Counting objects:  23% (44/190)\u001b[K\rremote: Counting objects:  24% (46/190)\u001b[K\rremote: Counting objects:  25% (48/190)\u001b[K\rremote: Counting objects:  26% (50/190)\u001b[K\rremote: Counting objects:  27% (52/190)\u001b[K\rremote: Counting objects:  28% (54/190)\u001b[K\rremote: Counting objects:  29% (56/190)\u001b[K\rremote: Counting objects:  30% (57/190)\u001b[K\rremote: Counting objects:  31% (59/190)\u001b[K\rremote: Counting objects:  32% (61/190)\u001b[K\rremote: Counting objects:  33% (63/190)\u001b[K\rremote: Counting objects:  34% (65/190)\u001b[K\rremote: Counting objects:  35% (67/190)\u001b[K\rremote: Counting objects:  36% (69/190)\u001b[K\rremote: Counting objects:  37% (71/190)\u001b[K\rremote: Counting objects:  38% (73/190)\u001b[K\rremote: Counting objects:  39% (75/190)\u001b[K\rremote: Counting objects:  40% (76/190)\u001b[K\rremote: Counting objects:  41% (78/190)\u001b[K\rremote: Counting objects:  42% (80/190)\u001b[K\rremote: Counting objects:  43% (82/190)\u001b[K\rremote: Counting objects:  44% (84/190)\u001b[K\rremote: Counting objects:  45% (86/190)\u001b[K\rremote: Counting objects:  46% (88/190)\u001b[K\rremote: Counting objects:  47% (90/190)\u001b[K\rremote: Counting objects:  48% (92/190)\u001b[K\rremote: Counting objects:  49% (94/190)\u001b[K\rremote: Counting objects:  50% (95/190)\u001b[K\rremote: Counting objects:  51% (97/190)\u001b[K\rremote: Counting objects:  52% (99/190)\u001b[K\rremote: Counting objects:  53% (101/190)\u001b[K\rremote: Counting objects:  54% (103/190)\u001b[K\rremote: Counting objects:  55% (105/190)\u001b[K\rremote: Counting objects:  56% (107/190)\u001b[K\rremote: Counting objects:  57% (109/190)\u001b[K\rremote: Counting objects:  58% (111/190)\u001b[K\rremote: Counting objects:  59% (113/190)\u001b[K\rremote: Counting objects:  60% (114/190)\u001b[K\rremote: Counting objects:  61% (116/190)\u001b[K\rremote: Counting objects:  62% (118/190)\u001b[K\rremote: Counting objects:  63% (120/190)\u001b[K\rremote: Counting objects:  64% (122/190)\u001b[K\rremote: Counting objects:  65% (124/190)\u001b[K\rremote: Counting objects:  66% (126/190)\u001b[K\rremote: Counting objects:  67% (128/190)\u001b[K\rremote: Counting objects:  68% (130/190)\u001b[K\rremote: Counting objects:  69% (132/190)\u001b[K\rremote: Counting objects:  70% (133/190)\u001b[K\rremote: Counting objects:  71% (135/190)\u001b[K\rremote: Counting objects:  72% (137/190)\u001b[K\rremote: Counting objects:  73% (139/190)\u001b[K\rremote: Counting objects:  74% (141/190)\u001b[K\rremote: Counting objects:  75% (143/190)\u001b[K\rremote: Counting objects:  76% (145/190)\u001b[K\rremote: Counting objects:  77% (147/190)\u001b[K\rremote: Counting objects:  78% (149/190)\u001b[K\rremote: Counting objects:  79% (151/190)\u001b[K\rremote: Counting objects:  80% (152/190)\u001b[K\rremote: Counting objects:  81% (154/190)\u001b[K\rremote: Counting objects:  82% (156/190)\u001b[K\rremote: Counting objects:  83% (158/190)\u001b[K\rremote: Counting objects:  84% (160/190)\u001b[K\rremote: Counting objects:  85% (162/190)\u001b[K\rremote: Counting objects:  86% (164/190)\u001b[K\rremote: Counting objects:  87% (166/190)\u001b[K\rremote: Counting objects:  88% (168/190)\u001b[K\rremote: Counting objects:  89% (170/190)\u001b[K\rremote: Counting objects:  90% (171/190)\u001b[K\rremote: Counting objects:  91% (173/190)\u001b[K\rremote: Counting objects:  92% (175/190)\u001b[K\rremote: Counting objects:  93% (177/190)\u001b[K\rremote: Counting objects:  94% (179/190)\u001b[K\rremote: Counting objects:  95% (181/190)\u001b[K\rremote: Counting objects:  96% (183/190)\u001b[K\rremote: Counting objects:  97% (185/190)\u001b[K\rremote: Counting objects:  98% (187/190)\u001b[K\rremote: Counting objects:  99% (189/190)\u001b[K\rremote: Counting objects: 100% (190/190)\u001b[K\rremote: Counting objects: 100% (190/190), done.\u001b[K\n",
            "remote: Compressing objects: 100% (183/183), done.\u001b[K\n",
            "remote: Total 190 (delta 8), reused 180 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (190/190), 1.98 MiB | 13.92 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install langchain-community\n",
        "!pip install langchain-openai\n",
        "!pip install neo4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuMQfowpkyBR",
        "outputId": "30a351a1-912f-463d-9453-e8c2889fef66"
      },
      "id": "RuMQfowpkyBR",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.12 (from langchain-community)\n",
            "  Downloading langchain-0.3.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.25 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.25-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain<0.4.0,>=0.3.12->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.3.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.12->langchain-community) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain-community) (2.27.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.12-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.12-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.25-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
            "Downloading langchain_text_splitters-0.3.3-py3-none-any.whl (27 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.24\n",
            "    Uninstalling langchain-core-0.3.24:\n",
            "      Successfully uninstalled langchain-core-0.3.24\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.2\n",
            "    Uninstalling langchain-text-splitters-0.3.2:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.11\n",
            "    Uninstalling langchain-0.3.11:\n",
            "      Successfully uninstalled langchain-0.3.11\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.12 langchain-community-0.3.12 langchain-core-0.3.25 langchain-text-splitters-0.3.3 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.7.0 typing-inspect-0.9.0\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.3.25)\n",
            "Collecting openai<2.0.0,>=1.55.3 (from langchain-openai)\n",
            "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (4.66.6)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
            "Downloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.5\n",
            "    Uninstalling openai-1.54.5:\n",
            "      Successfully uninstalled openai-1.54.5\n",
            "Successfully installed langchain-openai-0.2.12 openai-1.58.1 tiktoken-0.8.0\n",
            "Collecting neo4j\n",
            "  Downloading neo4j-5.27.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2024.2)\n",
            "Downloading neo4j-5.27.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.7/301.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: neo4j\n",
            "Successfully installed neo4j-5.27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebcbf000-c1dc-4859-bf14-cbc84eb32053",
      "metadata": {
        "id": "ebcbf000-c1dc-4859-bf14-cbc84eb32053"
      },
      "source": [
        "### Import packages and set up Neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8570032f-9008-4d8a-8232-59866218e01e",
      "metadata": {
        "height": 336,
        "id": "8570032f-9008-4d8a-8232-59866218e01e"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Common data processing\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "# Langchain\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d4edcf01-cac6-4539-a8c3-db5b175a41b2",
      "metadata": {
        "height": 302,
        "id": "d4edcf01-cac6-4539-a8c3-db5b175a41b2"
      },
      "outputs": [],
      "source": [
        "# Load from environment\n",
        "load_dotenv('/content/.env', override=True)\n",
        "NEO4J_URI = os.getenv('NEO4J_URI')\n",
        "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
        "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
        "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE') or 'neo4j'\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "OPENAI_ENDPOINT = os.getenv('OPENAI_BASE_URL') + '/embeddings'\n",
        "\n",
        "# Global constants\n",
        "VECTOR_INDEX_NAME = 'malwaredb_chunks'\n",
        "VECTOR_NODE_LABEL = 'Chunk'\n",
        "VECTOR_SOURCE_PROPERTY = 'text'\n",
        "VECTOR_EMBEDDING_PROPERTY = 'textEmbedding'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text file"
      ],
      "metadata": {
        "id": "LX9nGuDSUcgx"
      },
      "id": "LX9nGuDSUcgx"
    },
    {
      "cell_type": "code",
      "source": [
        "file_name=\"/content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt\"\n",
        "# Leggi tutto il contenuto come stringa\n",
        "with open(file_name, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "vJVLBD5RUkuo"
      },
      "id": "vJVLBD5RUkuo",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bf3d09a3-25a2-4f92-b9cb-37a6c0e4c808",
      "metadata": {
        "id": "bf3d09a3-25a2-4f92-b9cb-37a6c0e4c808"
      },
      "source": [
        "### Split Form 10-K sections into chunks\n",
        "- Set up text splitter using LangChain\n",
        "- For now, split only the text from the \"item 1\" section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fd3e4843-686a-40f1-97d1-b4811084d1b1",
      "metadata": {
        "height": 115,
        "id": "fd3e4843-686a-40f1-97d1-b4811084d1b1"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 2000,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5acbbf0b-6d25-46ab-9539-fb38479ff4fe",
      "metadata": {
        "height": 30,
        "id": "5acbbf0b-6d25-46ab-9539-fb38479ff4fe"
      },
      "outputs": [],
      "source": [
        "item1_text_chunks = text_splitter.split_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c35dc351-48d7-4487-b311-7224eb880c68",
      "metadata": {
        "height": 30,
        "id": "c35dc351-48d7-4487-b311-7224eb880c68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c2a03a-d9ea-49ac-e818-0763ac109cc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "type(item1_text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fc769d72-c01c-4d7b-aa3f-b12e8acd99e6",
      "metadata": {
        "height": 30,
        "id": "fc769d72-c01c-4d7b-aa3f-b12e8acd99e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7dbe883-f21e-4d1d-ca54-031bff81d58b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(item1_text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "af610b46-68f3-4763-9ffb-eb452dc61acf",
      "metadata": {
        "height": 30,
        "id": "af610b46-68f3-4763-9ffb-eb452dc61acf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "a2bd1c13-c615-4e0f-de50-4157f9939404"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Operation\\nArachnophobia\\nCaught in the Spider’s Web\\n\\n\\n\\n\\nRich Barger | Cyber Squared Inc.\\nMike Oppenheim | FireEye Labs\\nChris Phillips | FireEye Labs\\n\\x0cContents\\nTeam Introduction....................................................................................................................................................... 1\\n\\nKey Findings.................................................................................................................................................................. 1\\n\\nSummary....................................................................................................................................................................... 1\\n\\nBackstory......................................................................................................................................................................2\\n\\nVPSNOC/Digital Linx/Tranchulas............................................................................................................................ 4\\n\\nTechnical Observations...............................................................................................................................................8\\n\\nConclusion....................................................................................................................................................................11\\n\\nAppendix A: Malware Details................................................................................................................................... 12\\n\\nAppendix B: MD5 Hashes and Malware Table........................................................................................................ 17\\n\\nAppendix C: VPSNOC Email Header Analysis....................................................................................................... 20\\n\\nAppendix D: Inconsistencies Observed................................................................................................................... 21'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "item1_text_chunks[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accf6522-52e3-441f-b0c3-c43d754ba55a",
      "metadata": {
        "id": "accf6522-52e3-441f-b0c3-c43d754ba55a"
      },
      "source": [
        "- Set up helper function to chunk all sections of the Form 10-K\n",
        "- You'll limit the number of chunks in each section to 20 to speed things up"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_form10k_data_from_file(file_name):\n",
        "    chunks_with_metadata = [] # use this to accumlate chunk records\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "      text = file.read()\n",
        "      text_chunks = text_splitter.split_text(text) # split the text into chunks\n",
        "      chunk_seq_id = 0\n",
        "      for chunk in text_chunks[:20]: # only take the first 20 chunks\n",
        "          form_id=file_name\n",
        "          # finally, construct a record with metadata and the chunk text\n",
        "          chunks_with_metadata.append({\n",
        "              'text': chunk,\n",
        "              # metadata from looping...\n",
        "              'chunkSeqId': chunk_seq_id,\n",
        "              # constructed metadata...\n",
        "              'formId': f'{form_id}', # pulled from the filename\n",
        "              'chunkId': f'{form_id}-chunk{chunk_seq_id:04d}',\n",
        "              'source': file_name,\n",
        "          })\n",
        "          chunk_seq_id += 1\n",
        "      print(f'\\tSplit into {chunk_seq_id} chunks')\n",
        "    return chunks_with_metadata"
      ],
      "metadata": {
        "id": "b9SV2r7KYDyi"
      },
      "id": "b9SV2r7KYDyi",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6696ea15-abf7-4f05-8fd2-8e91e5fc540c",
      "metadata": {
        "height": 30,
        "id": "6696ea15-abf7-4f05-8fd2-8e91e5fc540c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26fd7fe-5396-4f51-ecce-2d33962885a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tSplit into 20 chunks\n"
          ]
        }
      ],
      "source": [
        "first_file_chunks = split_form10k_data_from_file(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d8b42f97-911f-495d-8176-a10879d2aae1",
      "metadata": {
        "height": 30,
        "id": "d8b42f97-911f-495d-8176-a10879d2aae1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952d1841-4118-42fb-bafc-470ebd4d4a50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'Operation\\nArachnophobia\\nCaught in the Spider’s Web\\n\\n\\n\\n\\nRich Barger | Cyber Squared Inc.\\nMike Oppenheim | FireEye Labs\\nChris Phillips | FireEye Labs\\n\\x0cContents\\nTeam Introduction....................................................................................................................................................... 1\\n\\nKey Findings.................................................................................................................................................................. 1\\n\\nSummary....................................................................................................................................................................... 1\\n\\nBackstory......................................................................................................................................................................2\\n\\nVPSNOC/Digital Linx/Tranchulas............................................................................................................................ 4\\n\\nTechnical Observations...............................................................................................................................................8\\n\\nConclusion....................................................................................................................................................................11\\n\\nAppendix A: Malware Details................................................................................................................................... 12\\n\\nAppendix B: MD5 Hashes and Malware Table........................................................................................................ 17\\n\\nAppendix C: VPSNOC Email Header Analysis....................................................................................................... 20\\n\\nAppendix D: Inconsistencies Observed................................................................................................................... 21',\n",
              " 'chunkSeqId': 0,\n",
              " 'formId': '/content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt',\n",
              " 'chunkId': '/content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0000',\n",
              " 'source': '/content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "first_file_chunks[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58952e6c-7e9c-44eb-b52b-bb11245738a4",
      "metadata": {
        "id": "58952e6c-7e9c-44eb-b52b-bb11245738a4"
      },
      "source": [
        "### Create graph nodes using text chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "968783b2-5c17-46cc-abfd-b6b5dfffe3bb",
      "metadata": {
        "height": 234,
        "id": "968783b2-5c17-46cc-abfd-b6b5dfffe3bb"
      },
      "outputs": [],
      "source": [
        "merge_chunk_node_query = \"\"\"\n",
        "MERGE(mergedChunk:Chunk {chunkId: $chunkParam.chunkId})\n",
        "    ON CREATE SET\n",
        "        mergedChunk.formId = $chunkParam.formId,\n",
        "        mergedChunk.chunkSeqId = $chunkParam.chunkSeqId,\n",
        "        mergedChunk.source = $chunkParam.source,\n",
        "        mergedChunk.text = $chunkParam.text\n",
        "RETURN mergedChunk\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34efd2c9-ad31-440a-92c5-9394465950f9",
      "metadata": {
        "id": "34efd2c9-ad31-440a-92c5-9394465950f9"
      },
      "source": [
        "- Set up connection to graph instance using LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f3f43dab-155e-4172-959b-e05968c0c6a8",
      "metadata": {
        "height": 64,
        "id": "f3f43dab-155e-4172-959b-e05968c0c6a8"
      },
      "outputs": [],
      "source": [
        "kg = Neo4jGraph(\n",
        "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddb150bf-1f77-4a5b-be0d-23ab86e32385",
      "metadata": {
        "id": "ddb150bf-1f77-4a5b-be0d-23ab86e32385"
      },
      "source": [
        "- Create a single chunk node for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6abdc4a7-8fb9-4cd9-8a79-8066ad67f618",
      "metadata": {
        "height": 47,
        "id": "6abdc4a7-8fb9-4cd9-8a79-8066ad67f618",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e0ec88-d8b6-4eec-d0d8-aba8c6c478c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'mergedChunk': {'formId': '/content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt',\n",
              "   'source': '/content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt',\n",
              "   'text': 'Operation\\nArachnophobia\\nCaught in the Spider’s Web\\n\\n\\n\\n\\nRich Barger | Cyber Squared Inc.\\nMike Oppenheim | FireEye Labs\\nChris Phillips | FireEye Labs\\n\\x0cContents\\nTeam Introduction....................................................................................................................................................... 1\\n\\nKey Findings.................................................................................................................................................................. 1\\n\\nSummary....................................................................................................................................................................... 1\\n\\nBackstory......................................................................................................................................................................2\\n\\nVPSNOC/Digital Linx/Tranchulas............................................................................................................................ 4\\n\\nTechnical Observations...............................................................................................................................................8\\n\\nConclusion....................................................................................................................................................................11\\n\\nAppendix A: Malware Details................................................................................................................................... 12\\n\\nAppendix B: MD5 Hashes and Malware Table........................................................................................................ 17\\n\\nAppendix C: VPSNOC Email Header Analysis....................................................................................................... 20\\n\\nAppendix D: Inconsistencies Observed................................................................................................................... 21',\n",
              "   'chunkId': '/content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0000',\n",
              "   'chunkSeqId': 0}}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "kg.query(merge_chunk_node_query,\n",
        "         params={'chunkParam':first_file_chunks[0]})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b368dd5-6e50-46db-b500-29993895631e",
      "metadata": {
        "id": "4b368dd5-6e50-46db-b500-29993895631e"
      },
      "source": [
        "- Create a uniqueness constraint to avoid duplicate chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "f8269f83-eb19-4a3f-979c-8b41b1e9f750",
      "metadata": {
        "height": 98,
        "id": "f8269f83-eb19-4a3f-979c-8b41b1e9f750",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8062fc00-a3c8-422f-ff8e-45417e60030e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "kg.query(\"\"\"\n",
        "CREATE CONSTRAINT unique_chunk IF NOT EXISTS\n",
        "    FOR (c:Chunk) REQUIRE c.chunkId IS UNIQUE\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "4e207e5f-eb17-452c-9c65-4846d3bbf840",
      "metadata": {
        "height": 30,
        "id": "4e207e5f-eb17-452c-9c65-4846d3bbf840",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e2c75d-7e69-4afb-d088-e886ce8b4a79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 0,\n",
              "  'name': 'index_343aff4e',\n",
              "  'state': 'ONLINE',\n",
              "  'populationPercent': 100.0,\n",
              "  'type': 'LOOKUP',\n",
              "  'entityType': 'NODE',\n",
              "  'labelsOrTypes': None,\n",
              "  'properties': None,\n",
              "  'indexProvider': 'token-lookup-1.0',\n",
              "  'owningConstraint': None,\n",
              "  'lastRead': None,\n",
              "  'readCount': 0},\n",
              " {'id': 1,\n",
              "  'name': 'index_f7700477',\n",
              "  'state': 'ONLINE',\n",
              "  'populationPercent': 100.0,\n",
              "  'type': 'LOOKUP',\n",
              "  'entityType': 'RELATIONSHIP',\n",
              "  'labelsOrTypes': None,\n",
              "  'properties': None,\n",
              "  'indexProvider': 'token-lookup-1.0',\n",
              "  'owningConstraint': None,\n",
              "  'lastRead': None,\n",
              "  'readCount': 0},\n",
              " {'id': 2,\n",
              "  'name': 'unique_chunk',\n",
              "  'state': 'ONLINE',\n",
              "  'populationPercent': 100.0,\n",
              "  'type': 'RANGE',\n",
              "  'entityType': 'NODE',\n",
              "  'labelsOrTypes': ['Chunk'],\n",
              "  'properties': ['chunkId'],\n",
              "  'indexProvider': 'range-1.0',\n",
              "  'owningConstraint': 'unique_chunk',\n",
              "  'lastRead': None,\n",
              "  'readCount': None}]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "kg.query(\"SHOW INDEXES\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b5c465e-c43f-422d-a3a1-17705a1019d5",
      "metadata": {
        "id": "1b5c465e-c43f-422d-a3a1-17705a1019d5"
      },
      "source": [
        "- Loop through and create nodes for all chunks\n",
        "- Should create 23 nodes because you set a limit of 20 chunks in the text splitting function above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "e0e4f598-a6b8-419f-b247-4eecd1f3d98a",
      "metadata": {
        "height": 166,
        "id": "e0e4f598-a6b8-419f-b247-4eecd1f3d98a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8c09ef-85a5-4394-ea31-c7cea0472503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0000\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0001\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0002\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0003\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0004\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0005\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0006\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0007\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0008\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0009\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0010\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0011\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0012\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0013\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0014\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0015\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0016\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0017\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0018\n",
            "Creating `:Chunk` node for chunk ID /content/CyberSA-RAG/P17-1143.Datasets/dataset/plaintext/ThreatConnect_Operation_Arachnophobia_Report.pdf.txt-chunk0019\n",
            "Created 20 nodes\n"
          ]
        }
      ],
      "source": [
        "node_count = 0\n",
        "for chunk in first_file_chunks:\n",
        "    print(f\"Creating `:Chunk` node for chunk ID {chunk['chunkId']}\")\n",
        "    kg.query(merge_chunk_node_query,\n",
        "            params={\n",
        "                'chunkParam': chunk\n",
        "            })\n",
        "    node_count += 1\n",
        "print(f\"Created {node_count} nodes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "5d820104-51be-4709-b531-2bdc9b6d5eec",
      "metadata": {
        "height": 81,
        "id": "5d820104-51be-4709-b531-2bdc9b6d5eec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbaeb625-005f-4750-e4ba-df7fbca86822"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'nodeCount': 20}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "kg.query(\"\"\"\n",
        "         MATCH (n)\n",
        "         RETURN count(n) as nodeCount\n",
        "         \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c46b6bfa-9e07-4963-9b3f-50cdba4468bc",
      "metadata": {
        "id": "c46b6bfa-9e07-4963-9b3f-50cdba4468bc"
      },
      "source": [
        "### Create a vector index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "79355493-3790-4b05-898b-a3eaaea87155",
      "metadata": {
        "height": 149,
        "id": "79355493-3790-4b05-898b-a3eaaea87155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab0dc3f8-d113-49b9-bef4-fcd45ef068f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "kg.query(\"\"\"\n",
        "         CREATE VECTOR INDEX `malwaredb_chunks` IF NOT EXISTS\n",
        "          FOR (c:Chunk) ON (c.textEmbedding)\n",
        "          OPTIONS { indexConfig: {\n",
        "            `vector.dimensions`: 1536,\n",
        "            `vector.similarity_function`: 'cosine'\n",
        "         }}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "507b42c0-a90c-4f97-8c8a-b8a4ec70f9d1",
      "metadata": {
        "height": 30,
        "id": "507b42c0-a90c-4f97-8c8a-b8a4ec70f9d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff058ffc-d28c-46de-d80f-7445382935b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 0,\n",
              "  'name': 'index_343aff4e',\n",
              "  'state': 'ONLINE',\n",
              "  'populationPercent': 100.0,\n",
              "  'type': 'LOOKUP',\n",
              "  'entityType': 'NODE',\n",
              "  'labelsOrTypes': None,\n",
              "  'properties': None,\n",
              "  'indexProvider': 'token-lookup-1.0',\n",
              "  'owningConstraint': None,\n",
              "  'lastRead': None,\n",
              "  'readCount': 0},\n",
              " {'id': 1,\n",
              "  'name': 'index_f7700477',\n",
              "  'state': 'ONLINE',\n",
              "  'populationPercent': 100.0,\n",
              "  'type': 'LOOKUP',\n",
              "  'entityType': 'RELATIONSHIP',\n",
              "  'labelsOrTypes': None,\n",
              "  'properties': None,\n",
              "  'indexProvider': 'token-lookup-1.0',\n",
              "  'owningConstraint': None,\n",
              "  'lastRead': None,\n",
              "  'readCount': 0},\n",
              " {'id': 4,\n",
              "  'name': 'malwaredb_chunks',\n",
              "  'state': 'ONLINE',\n",
              "  'populationPercent': 100.0,\n",
              "  'type': 'VECTOR',\n",
              "  'entityType': 'NODE',\n",
              "  'labelsOrTypes': ['Chunk'],\n",
              "  'properties': ['textEmbedding'],\n",
              "  'indexProvider': 'vector-2.0',\n",
              "  'owningConstraint': None,\n",
              "  'lastRead': None,\n",
              "  'readCount': None},\n",
              " {'id': 2,\n",
              "  'name': 'unique_chunk',\n",
              "  'state': 'ONLINE',\n",
              "  'populationPercent': 100.0,\n",
              "  'type': 'RANGE',\n",
              "  'entityType': 'NODE',\n",
              "  'labelsOrTypes': ['Chunk'],\n",
              "  'properties': ['chunkId'],\n",
              "  'indexProvider': 'range-1.0',\n",
              "  'owningConstraint': 'unique_chunk',\n",
              "  'lastRead': neo4j.time.DateTime(2024, 12, 18, 12, 22, 40, 355000000, tzinfo=<UTC>),\n",
              "  'readCount': 58}]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "kg.query(\"SHOW INDEXES\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b9e379f-69d1-41de-8443-c6f5660603ab",
      "metadata": {
        "id": "2b9e379f-69d1-41de-8443-c6f5660603ab"
      },
      "source": [
        "### Calculate embedding vectors for chunks and populate index\n",
        "- This query calculates the embedding vector and stores it as a property called `textEmbedding` on each `Chunk` node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "43f2d4f2-ea0b-4731-8b29-a858e05f5b36",
      "metadata": {
        "height": 217,
        "id": "43f2d4f2-ea0b-4731-8b29-a858e05f5b36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c9be58-8f58-44d2-9e0e-8af4d93d3632"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "kg.query(\"\"\"\n",
        "    MATCH (chunk:Chunk) WHERE chunk.textEmbedding IS NULL\n",
        "    WITH chunk, genai.vector.encode(\n",
        "      chunk.text,\n",
        "      \"OpenAI\",\n",
        "      {\n",
        "        token: $openAiApiKey,\n",
        "        endpoint: $openAiEndpoint\n",
        "      }) AS vector\n",
        "    CALL db.create.setNodeVectorProperty(chunk, \"textEmbedding\", vector)\n",
        "    \"\"\",\n",
        "    params={\"openAiApiKey\":OPENAI_API_KEY, \"openAiEndpoint\": OPENAI_ENDPOINT} )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "59550c1c-5d7f-4e87-b5fc-ae7a4edb3731",
      "metadata": {
        "height": 47,
        "id": "59550c1c-5d7f-4e87-b5fc-ae7a4edb3731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a31e2f-5c90-46b9-fb07-4aa24f5cd8db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node properties:\n",
            "Chunk {chunkId: STRING, formId: STRING, chunkSeqId: INTEGER, source: STRING, text: STRING, textEmbedding: LIST}\n",
            "Relationship properties:\n",
            "\n",
            "The relationships:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "kg.refresh_schema()\n",
        "print(kg.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d5286ca-106c-49ec-8f60-f142db8bb680",
      "metadata": {
        "id": "4d5286ca-106c-49ec-8f60-f142db8bb680"
      },
      "source": [
        "### Use similarity search to find relevant chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3ab0d7b-7be2-4a2b-8b2c-2ea8a35350bd",
      "metadata": {
        "id": "a3ab0d7b-7be2-4a2b-8b2c-2ea8a35350bd"
      },
      "source": [
        "- Setup a help function to perform similarity search using the vector index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "50153721-d565-47ac-b288-d8b9798d9479",
      "metadata": {
        "height": 251,
        "id": "50153721-d565-47ac-b288-d8b9798d9479"
      },
      "outputs": [],
      "source": [
        "def neo4j_vector_search(question):\n",
        "  \"\"\"Search for similar nodes using the Neo4j vector index\"\"\"\n",
        "  vector_search_query = \"\"\"\n",
        "    WITH genai.vector.encode(\n",
        "      $question,\n",
        "      \"OpenAI\",\n",
        "      {\n",
        "        token: $openAiApiKey,\n",
        "        endpoint: $openAiEndpoint\n",
        "      }) AS question_embedding\n",
        "    CALL db.index.vector.queryNodes($index_name, $top_k, question_embedding) yield node, score\n",
        "    RETURN score, node.text AS text\n",
        "  \"\"\"\n",
        "  similar = kg.query(vector_search_query,\n",
        "                     params={\n",
        "                      'question': question,\n",
        "                      'openAiApiKey':OPENAI_API_KEY,\n",
        "                      'openAiEndpoint': OPENAI_ENDPOINT,\n",
        "                      'index_name':VECTOR_INDEX_NAME,\n",
        "                      'top_k': 10})\n",
        "  return similar"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aba0f292-59e8-432c-b521-2a77613b08c6",
      "metadata": {
        "id": "aba0f292-59e8-432c-b521-2a77613b08c6"
      },
      "source": [
        "- Ask a question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "6ae9975a-d26f-4477-92ac-8dcfb3119c47",
      "metadata": {
        "height": 64,
        "id": "6ae9975a-d26f-4477-92ac-8dcfb3119c47"
      },
      "outputs": [],
      "source": [
        "search_results = neo4j_vector_search(\n",
        "    'What evidence links BITTERBUG activity to Pakistan-based entities?'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "a2370fb5-9990-42e3-9ac2-0edf03969705",
      "metadata": {
        "height": 30,
        "id": "a2370fb5-9990-42e3-9ac2-0edf03969705",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45abe1ea-6cbf-4029-e68c-f843a593a6fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.919342041015625,\n",
              " 'text': 'Digital Appendix 4: Maltego Visualization.............................................................................................................35\\n\\n\\n\\n\\ni    •     OPERATION ARACHNOPHOBIA\\n\\x0cTeam Introduction\\nCyber Squared Inc.’s ThreatConnect Intelligence Research Team (TCIRT) tracks a number of threat groups around the world.\\nBeginning in the summer of 2013, TCIRT identified a suspected Pakistani-origin threat group. This group was revealed by TCIRT\\npublicly in August 2013. In the months following the disclosure, we identified new activity. Cyber Squared partnered with experts\\nat FireEye Labs to examine these new observations in an attempt to discover new research and insight into the group and its\\nOperation “Arachnophobia”. The following report is a product of collaborative research and threat intelligence sharing between\\nCyber Squared Inc.’s TCIRT and FireEye Labs.\\n\\n\\nKey Findings\\n•\\t While we are not conclusively attributing BITTERBUG activity to Tranchulas or a specific Pakistani entity, we can confidently\\n    point to many characteristics of a Pakistan-based cyber exploitation effort that is probably directed against Indian targets and/or\\n    those who are involved in India-Pakistan issues.\\n\\n•\\t The threat actors utilized a hosting provider that is a Pakistani-based company with subleased VPS space within the U.S. for\\n    command and control (C2).\\n\\n•\\t The customized malware (BITTERBUG) used by these threat actors has only been observed hosted on and communicating with\\n    two IP addresses operated by a Pakistan-based hosting provider.\\n\\n•\\t Early variants of the BITTERBUG malware had build paths containing the strings “Tranchulas” and “umairaziz27”.\\n    Tranchulas is the name of a Pakistani security firm; Umair Aziz is the name of a Tranchulas employee.\\n\\n•\\t Following the release of our blog post highlighting this activity and the malware’s build strings, the threat actors appear to have\\n    modified their binary file paths to make them more generic.'}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "search_results[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5777c9ea-ca26-4690-8487-6aec06db1d4c",
      "metadata": {
        "id": "5777c9ea-ca26-4690-8487-6aec06db1d4c"
      },
      "source": [
        "### Set up a LangChain RAG workflow to chat with the form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "17cdab04-4db1-4776-bd89-f4f87b57bde4",
      "metadata": {
        "height": 200,
        "id": "17cdab04-4db1-4776-bd89-f4f87b57bde4"
      },
      "outputs": [],
      "source": [
        "neo4j_vector_store = Neo4jVector.from_existing_graph(\n",
        "    embedding=OpenAIEmbeddings(),\n",
        "    url=NEO4J_URI,\n",
        "    username=NEO4J_USERNAME,\n",
        "    password=NEO4J_PASSWORD,\n",
        "    index_name=VECTOR_INDEX_NAME,\n",
        "    node_label=VECTOR_NODE_LABEL,\n",
        "    text_node_properties=[VECTOR_SOURCE_PROPERTY],\n",
        "    embedding_node_property=VECTOR_EMBEDDING_PROPERTY,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "2a7abbd6-bf4e-4fc3-9c20-c4246ed3ec4e",
      "metadata": {
        "height": 30,
        "id": "2a7abbd6-bf4e-4fc3-9c20-c4246ed3ec4e"
      },
      "outputs": [],
      "source": [
        "retriever = neo4j_vector_store.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b38dd23-fa0f-4304-849a-80043e34a1e7",
      "metadata": {
        "id": "7b38dd23-fa0f-4304-849a-80043e34a1e7"
      },
      "source": [
        "- Set up a RetrievalQAWithSourcesChain to carry out question answering\n",
        "- You can check out the LangChain documentation for this chain [here](https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "47b45848-112f-4e4f-9c76-cf2bda9c6f8e",
      "metadata": {
        "height": 98,
        "id": "47b45848-112f-4e4f-9c76-cf2bda9c6f8e"
      },
      "outputs": [],
      "source": [
        "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    ChatOpenAI(temperature=0),\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "a5aee157-c777-4c9a-949b-c55005d19c41",
      "metadata": {
        "height": 98,
        "id": "a5aee157-c777-4c9a-949b-c55005d19c41"
      },
      "outputs": [],
      "source": [
        "def prettychain(question: str) -> str:\n",
        "    \"\"\"Pretty print the chain's response to a question\"\"\"\n",
        "    response = chain({\"question\": question},\n",
        "        return_only_outputs=True,)\n",
        "    print(textwrap.fill(response['answer'], 60))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb96301-66a2-47e9-87a7-13d926f85019",
      "metadata": {
        "id": "cbb96301-66a2-47e9-87a7-13d926f85019"
      },
      "source": [
        "- Ask a question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "1b234d29-7fc0-483f-bbfa-459617be96c1",
      "metadata": {
        "height": 30,
        "id": "1b234d29-7fc0-483f-bbfa-459617be96c1"
      },
      "outputs": [],
      "source": [
        "question = \"What evidence links BITTERBUG activity to Pakistan-based entities?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "2a2228aa-2af0-44cd-99b0-bffdc72de62b",
      "metadata": {
        "height": 30,
        "id": "2a2228aa-2af0-44cd-99b0-bffdc72de62b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea5813c-878f-438a-e004-1d447cdd6d86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The evidence linking BITTERBUG activity to Pakistan-based\n",
            "entities includes the use of a Pakistani-based hosting\n",
            "provider for command and control, the hosting of malware on\n",
            "IP addresses operated by a Pakistan-based hosting provider,\n",
            "and connections between employees of the hosting provider\n",
            "and Tranchulas, a Pakistani security firm. Additionally,\n",
            "early variants of the BITTERBUG malware contained build\n",
            "paths referencing Tranchulas and an employee of the firm\n",
            "named Umair Aziz.\n"
          ]
        }
      ],
      "source": [
        "prettychain(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "e2fa5b5b-5ec6-4903-bb53-acf8bd5f49c3",
      "metadata": {
        "height": 30,
        "id": "e2fa5b5b-5ec6-4903-bb53-acf8bd5f49c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d6ef7c-debf-4676-ea9d-b93dd845f409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Organizations can better protect themselves against similar\n",
            "threats by employing personnel with offensive cyber\n",
            "expertise and closely monitoring any suspicious activities.\n",
            "It is also important to maintain open communication with\n",
            "hosting providers and promptly address any inconsistencies\n",
            "in claims or responses. Regularly reviewing and analyzing\n",
            "malware associated with potential threats can also help\n",
            "organizations identify vulnerabilities and take necessary\n",
            "precautions.\n"
          ]
        }
      ],
      "source": [
        "prettychain(\"How can organizations better protect themselves against similar threats?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "758bdd8e-90ca-4267-aef2-db33b28937b6",
      "metadata": {
        "height": 81,
        "id": "758bdd8e-90ca-4267-aef2-db33b28937b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08fe07b6-7c5a-4b49-d325-c70e7b75eda8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The evidence linking BITTERBUG activity to Pakistan-based\n",
            "entities includes the use of a Pakistani-based hosting\n",
            "provider for command and control, communication with IP\n",
            "addresses operated by a Pakistan-based hosting provider, and\n",
            "early variants of the malware containing references to\n",
            "\"Tranchulas\" and \"umairaziz27.\"\n"
          ]
        }
      ],
      "source": [
        "prettychain(\"\"\"\n",
        "    What evidence links BITTERBUG activity to Pakistan-based entities.\n",
        "    Limit your answer to a single sentence.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "10715705-25a7-4c21-88e2-bc540c72dacb",
      "metadata": {
        "height": 81,
        "id": "10715705-25a7-4c21-88e2-bc540c72dacb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10fc3f9c-ea17-422d-9373-16cb1b33a36d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple is not mentioned in the provided content.\n"
          ]
        }
      ],
      "source": [
        "prettychain(\"\"\"\n",
        "    Tell me about Apple.\n",
        "    Limit your answer to a single sentence.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "53300764-c93b-4127-9bbd-9e17428d066c",
      "metadata": {
        "height": 98,
        "id": "53300764-c93b-4127-9bbd-9e17428d066c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5596de11-ca13-4ea2-d1bf-6b5132f91d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know.\n"
          ]
        }
      ],
      "source": [
        "prettychain(\"\"\"\n",
        "    Tell me about Apple.\n",
        "    Limit your answer to a single sentence.\n",
        "    If you are unsure about the answer, say you don't know.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ab2124-77c4-496b-b951-44c9466fe941",
      "metadata": {
        "id": "c1ab2124-77c4-496b-b951-44c9466fe941"
      },
      "source": [
        "### Ask you own question!\n",
        "- Add your own question to the call to prettychain below to find out more about NetApp\n",
        "- Here is NetApp's website if you want some inspiration: https://www.netapp.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f238eaec-7080-4e45-80f0-5b37e2340387",
      "metadata": {
        "height": 64,
        "id": "f238eaec-7080-4e45-80f0-5b37e2340387"
      },
      "outputs": [],
      "source": [
        "prettychain(\"\"\"\n",
        "    ADD YOUR OWN QUESTION HERE\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}